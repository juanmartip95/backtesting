{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fbaf4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 06.5_ensemble_portfolio_allocation.ipynb\n",
    "\n",
    "# Cell 1: Import Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from ta.momentum import RSIIndicator\n",
    "from ta.trend import MACD\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import classification_report, roc_auc_score, make_scorer\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from alpha_vantage.timeseries import TimeSeries\n",
    "from alpha_vantage.techindicators import TechIndicators\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import requests # For direct API call to Alpha Vantage News\n",
    "from scipy.optimize import minimize\n",
    "import json # For loading selected tickers\n",
    "\n",
    "# Load environment variables from .env\n",
    "load_dotenv()\n",
    "\n",
    "# Get the API key\n",
    "API_KEY = os.getenv(\"ALPHA_VANTAGE_API_KEY\")\n",
    "\n",
    "# Define target labels for multi-class classification\n",
    "LABELS = [-1, 0, 1]\n",
    "\n",
    "# Create a custom multi-class ROC AUC scorer for GridSearchCV\n",
    "multi_class_roc_auc_scorer = make_scorer(roc_auc_score, needs_proba=True, multi_class='ovr', labels=LABELS)\n",
    "\n",
    "print(\"âœ… Libraries imported and global variables defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e980d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Define Functions for Data Fetching and Individual Ticker Signal Generation\n",
    "\n",
    "def fetch_financial_data(symbol, api_key):\n",
    "    \"\"\"Fetches daily adjusted financial data for a given symbol.\"\"\"\n",
    "    ts = TimeSeries(key=api_key, output_format='pandas')\n",
    "    data, _ = ts.get_daily_adjusted(symbol=symbol, outputsize='full')\n",
    "    data.columns = ['open', 'high', 'low', 'close', 'adjusted_close', 'volume', 'dividend_amount', 'split_coefficient']\n",
    "    data.index = pd.to_datetime(data.index)\n",
    "    data.sort_index(inplace=True)\n",
    "    return data\n",
    "\n",
    "def fetch_news_sentiment(symbol, api_key, start_date=None, end_date=None):\n",
    "    \"\"\"\n",
    "    Fetches daily news sentiment for a given symbol from Alpha Vantage.\n",
    "    Note: Alpha Vantage free tier has limits on historical data and rate limits.\n",
    "    \"\"\"\n",
    "    news_data_list = []\n",
    "    # Set default date range if not provided. Adjust as per your needs.\n",
    "    if start_date is None:\n",
    "        start_date = (pd.Timestamp.now() - pd.DateOffset(years=2)).strftime('%Y%m%dT%H%M')\n",
    "    else:\n",
    "        start_date = pd.to_datetime(start_date).strftime('%Y%m%dT%H%M')\n",
    "    if end_date is None:\n",
    "        end_date = pd.Timestamp.now().strftime('%Y%m%dT%H%M')\n",
    "    else:\n",
    "        end_date = pd.to_datetime(end_date).strftime('%Y%m%dT%H%M')\n",
    "\n",
    "    news_url = f\"https://www.alphavantage.co/query?function=NEWS_SENTIMENT&tickers={symbol}&time_from={start_date}&time_to={end_date}&apikey={api_key}\"\n",
    "    response = requests.get(news_url)\n",
    "    json_data = response.json()\n",
    "\n",
    "    if 'feed' in json_data:\n",
    "        for article in json_data['feed']:\n",
    "            pub_time = pd.to_datetime(article['time_published'], format='%Y%m%dT%H%M%S', errors='coerce')\n",
    "            if pd.isna(pub_time):\n",
    "                pub_time = pd.to_datetime(article['time_published'], format='%Y%m%dT%H%M', errors='coerce')\n",
    "\n",
    "            target_ticker_sentiment = None\n",
    "            if 'ticker_sentiment' in article:\n",
    "                for ts_entry in article['ticker_sentiment']:\n",
    "                    if ts_entry.get('ticker') == symbol:\n",
    "                        target_ticker_sentiment = float(ts_entry.get('ticker_sentiment_score', 0.0))\n",
    "                        break\n",
    "            if target_ticker_sentiment is not None:\n",
    "                 news_data_list.append({\n",
    "                     'date': pub_time.date(),\n",
    "                     'sentiment': target_ticker_sentiment\n",
    "                 })\n",
    "\n",
    "    if news_data_list:\n",
    "        news_df = pd.DataFrame(news_data_list)\n",
    "        news_df['date'] = pd.to_datetime(news_df['date'])\n",
    "        news_df.set_index('date', inplace=True)\n",
    "        news_df.sort_index(inplace=True)\n",
    "        daily_avg_sentiment = news_df.groupby(news_df.index).mean().rename(columns={'sentiment': 'daily_sentiment'})\n",
    "        return daily_avg_sentiment\n",
    "    else:\n",
    "        print(f\"âš ï¸ No news data or relevant news found for {symbol}. Returning empty DataFrame.\")\n",
    "        return pd.DataFrame(columns=['daily_sentiment'], index=pd.to_datetime([]))\n",
    "\n",
    "\n",
    "def add_features_and_target(df, daily_avg_sentiment_df):\n",
    "    \"\"\"Adds technical indicators, sentiment features, and target variable.\"\"\"\n",
    "    feature_data = df.copy()\n",
    "\n",
    "    # Financial Features\n",
    "    feature_data['daily_return'] = feature_data['close'].pct_change()\n",
    "    rsi_indicator = RSIIndicator(close=feature_data['close'], window=14)\n",
    "    feature_data['rsi'] = rsi_indicator.rsi()\n",
    "    macd_indicator = MACD(close=feature_data['close'])\n",
    "    feature_data['macd'] = macd_indicator.macd()\n",
    "    feature_data['macd_signal'] = macd_indicator.macd_signal()\n",
    "\n",
    "    # Sentiment Features\n",
    "    feature_data = feature_data.join(daily_avg_sentiment_df, how='left')\n",
    "    feature_data['daily_sentiment'].fillna(0, inplace=True) # Fill NaN for days with no news\n",
    "\n",
    "    feature_data['rolling_avg_sentiment_7d'] = feature_data['daily_sentiment'].rolling(window=7, min_periods=5).mean()\n",
    "    feature_data['rolling_std_sentiment_30d'] = feature_data['daily_sentiment'].rolling(window=30, min_periods=10).std()\n",
    "\n",
    "    STD_DEV_THRESHOLD = 1.5\n",
    "    feature_data['sentiment_deviation'] = abs(feature_data['daily_sentiment'] - feature_data['rolling_avg_sentiment_7d'])\n",
    "    feature_data['sentiment_shift_flag'] = np.where(\n",
    "        (feature_data['sentiment_deviation'] > (STD_DEV_THRESHOLD * feature_data['rolling_std_sentiment_30d'])) &\n",
    "        (feature_data['rolling_std_sentiment_30d'].notna()), 1, 0)\n",
    "\n",
    "    feature_data['sentiment_signal'] = 0\n",
    "    feature_data.loc[(feature_data['daily_sentiment'] > 0.5) & (feature_data['sentiment_shift_flag'] == 0), 'sentiment_signal'] = 1\n",
    "    feature_data.loc[(feature_data['daily_sentiment'] < -0.5) & (feature_data['sentiment_shift_flag'] == 0), 'sentiment_signal'] = -1\n",
    "    feature_data.loc[(feature_data['sentiment_shift_flag'] == 1) & (feature_data['daily_sentiment'] > 0), 'sentiment_signal'] = 1\n",
    "    feature_data.loc[(feature_data['sentiment_shift_flag'] == 1) & (feature_data['daily_sentiment'] < 0), 'sentiment_signal'] = -1\n",
    "\n",
    "    # Define Multi-Class Target Variable\n",
    "    feature_data['future_return'] = feature_data['close'].pct_change().shift(-1)\n",
    "    BUY_THRESHOLD = 0.001\n",
    "    SELL_THRESHOLD = -0.001\n",
    "    feature_data['target'] = 0\n",
    "    feature_data.loc[feature_data['future_return'] > BUY_THRESHOLD, 'target'] = 1\n",
    "    feature_data.loc[feature_data['future_return'] < SELL_THRESHOLD, 'target'] = -1\n",
    "    feature_data.drop(columns=['future_return'], inplace=True)\n",
    "\n",
    "    feature_data.dropna(inplace=True)\n",
    "    return feature_data\n",
    "\n",
    "\n",
    "def train_and_predict_models(feature_data_df):\n",
    "    \"\"\"Trains RF, XGB, SVM models and returns their predictions for the test set.\"\"\"\n",
    "    features = ['daily_return', 'rsi', 'macd', 'macd_signal',\n",
    "                'daily_sentiment', 'rolling_avg_sentiment_7d', 'sentiment_deviation', 'sentiment_shift_flag', 'sentiment_signal']\n",
    "    X = feature_data_df[features]\n",
    "    y = feature_data_df['target']\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False, random_state=42)\n",
    "\n",
    "    predictions = pd.DataFrame(index=X_test.index)\n",
    "\n",
    "    # RandomForest (Base)\n",
    "    rf_model = RandomForestClassifier(random_state=42, class_weight='balanced')\n",
    "    rf_model.fit(X_train, y_train)\n",
    "    predictions['y_pred_rf'] = rf_model.predict(X_test)\n",
    "\n",
    "    # RandomForest (Best - Simplified GridSearch for speed in multi-ticker loop)\n",
    "    param_grid_rf_simple = {\n",
    "        'n_estimators': [50, 100],\n",
    "        'max_depth': [5, 10],\n",
    "    }\n",
    "    rf_base = RandomForestClassifier(random_state=42, class_weight='balanced')\n",
    "    grid_search_rf = GridSearchCV(estimator=rf_base, param_grid=param_grid_rf_simple,\n",
    "                                  cv=3, scoring=multi_class_roc_auc_scorer, n_jobs=-1)\n",
    "    grid_search_rf.fit(X_train, y_train)\n",
    "    best_rf_model = grid_search_rf.best_estimator_\n",
    "    predictions['y_pred_best_rf'] = best_rf_model.predict(X_test)\n",
    "\n",
    "    # XGBoost\n",
    "    le = LabelEncoder()\n",
    "    y_train_encoded = le.fit_transform(y_train)\n",
    "    # Ensure y_test_encoded is properly transformed using the fitted encoder.\n",
    "    # Handle cases where y_test might have classes not in y_train\n",
    "    unique_y_test_classes = np.setdiff1d(y_test.unique(), le.classes_)\n",
    "    if len(unique_y_test_classes) > 0:\n",
    "        # Fallback if a class appears only in test set (shouldn't happen with shuffle=False)\n",
    "        # but robust coding suggests handling it. For now, assume good split.\n",
    "        pass\n",
    "    y_test_encoded = le.transform(y_test)\n",
    "\n",
    "    xgb_model = XGBClassifier(objective='multi:softmax', num_class=len(le.classes_),\n",
    "                              use_label_encoder=False, eval_metric='mlogloss', random_state=42)\n",
    "    xgb_model.fit(X_train, y_train_encoded)\n",
    "    xgb_pred_encoded = xgb_model.predict(X_test)\n",
    "    predictions['y_pred_xgb'] = le.inverse_transform(xgb_pred_encoded)\n",
    "\n",
    "    # SVM\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    X_train_scaled_df = pd.DataFrame(X_train_scaled, index=X_train.index, columns=X_train.columns)\n",
    "    X_test_scaled_df = pd.DataFrame(X_test_scaled, index=X_test.index, columns=X_test.columns)\n",
    "\n",
    "    svm_model = SVC(probability=True, random_state=42, class_weight='balanced')\n",
    "    svm_model.fit(X_train_scaled_df, y_train)\n",
    "    predictions['y_pred_svm'] = svm_model.predict(X_test_scaled_df)\n",
    "\n",
    "    # Classical Signals (re-calculate on the whole feature_data_df then slice for test set)\n",
    "    feature_data_df['SMA_10'] = feature_data_df['close'].rolling(window=10).mean()\n",
    "    feature_data_df['SMA_50'] = feature_data_df['close'].rolling(window=50).mean()\n",
    "    feature_data_df['y_pred_crossover_full'] = np.where(feature_data_df['SMA_10'] > feature_data_df['SMA_50'], 1, 0)\n",
    "    feature_data_df['y_pred_rsi_full'] = np.where(feature_data_df['rsi'] < 30, 1, 0)\n",
    "    feature_data_df['y_pred_rsi_full'] = np.where(feature_data_df['rsi'] > 70, -1, feature_data_df['y_pred_rsi_full']) # Use -1 for sell\n",
    "    \n",
    "    # Align classical signals with the test set index\n",
    "    predictions['y_pred_crossover'] = feature_data_df['y_pred_crossover_full'].loc[X_test.index]\n",
    "    predictions['y_pred_rsi'] = feature_data_df['y_pred_rsi_full'].loc[X_test.index]\n",
    "\n",
    "    return predictions, X_test.index # Return the test set index to align market returns later\n",
    "\n",
    "print(\"âœ… Functions for data fetching, feature engineering, and model prediction defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb626578",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Define Portfolio Optimization Functions (from 06_portfolio_optimization.ipynb)\n",
    "\n",
    "def annual_return(returns, annual_factor=252):\n",
    "    \"\"\"Calculates the annualized returns (geometric mean).\"\"\"\n",
    "    return (1 + returns).prod()**(annual_factor / len(returns)) - 1\n",
    "\n",
    "def sharpe_ratio(returns, risk_free_rate=0.0, annual_factor=252):\n",
    "    \"\"\"Calculates the annualized Sharpe Ratio.\"\"\"\n",
    "    if returns.std() == 0:\n",
    "        return 0.0\n",
    "    daily_risk_free_rate = (1 + risk_free_rate)**(1/annual_factor) - 1\n",
    "    excess_returns = returns - daily_risk_free_rate\n",
    "    return (excess_returns.mean() * annual_factor) / \\\n",
    "           (excess_returns.std() * np.sqrt(annual_factor))\n",
    "\n",
    "def combined_objective(weights, returns_matrix, alpha, risk_free_rate=0.0):\n",
    "    \"\"\"Objective function for combined Sharpe and Annual Return optimization.\"\"\"\n",
    "    portfolio_returns = returns_matrix.dot(weights)\n",
    "    sharpe = sharpe_ratio(portfolio_returns, risk_free_rate=risk_free_rate)\n",
    "    cagr = annual_return(portfolio_returns)\n",
    "    # Maximize (alpha * Sharpe + (1 - alpha) * CAGR) -> Minimize negative of this\n",
    "    return - (alpha * sharpe + (1 - alpha) * cagr)\n",
    "\n",
    "def optimize_combined_strategy(returns_matrix, alpha, risk_free_rate=0.0):\n",
    "    \"\"\"Optimizes portfolio weights based on combined Sharpe and Annual Return.\"\"\"\n",
    "    n = returns_matrix.shape[1]\n",
    "    init_guess = np.ones(n) / n\n",
    "    bounds = [(0.0, 1.0)] * n # Weights between 0 and 1\n",
    "    constraints = {'type': 'eq', 'fun': lambda w: np.sum(w) - 1} # Weights sum to 1\n",
    "\n",
    "    result = minimize(combined_objective, init_guess,\n",
    "                      args=(returns_matrix, alpha, risk_free_rate),\n",
    "                      method='SLSQP',\n",
    "                      bounds=bounds,\n",
    "                      constraints=constraints)\n",
    "    # The objective returns negative value, so -result.fun gives the actual score\n",
    "    return result.x, -result.fun\n",
    "\n",
    "print(\"âœ… Portfolio optimization functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550c933d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Select Tickers\n",
    "example_tickers = ['AAPL', 'MSFT', 'GOOG', 'AMZN', 'META', 'TSLA', 'NFLX', 'NVDA', 'JPM', 'BA']\n",
    "\n",
    "print(\"ðŸ”¢ Example Tickers:\")\n",
    "for i, ticker in enumerate(example_tickers):\n",
    "    print(f\"{i+1}. {ticker}\")\n",
    "\n",
    "# You can either let the user choose or hardcode for faster testing\n",
    "# chosen_indices = input(\"Please choose 5 ticker indices (comma-separated, e.g. 1,3,5,7,9): \")\n",
    "# chosen_tickers = [example_tickers[int(i)-1] for i in chosen_indices.split(',')]\n",
    "chosen_tickers = ['AAPL', 'MSFT', 'GOOG', 'AMZN', 'META'] # Hardcoded for demonstration\n",
    "\n",
    "print(f\"\\nâœ… You selected: {chosen_tickers}\")\n",
    "\n",
    "# Save to file for reuse (optional, but good for consistency)\n",
    "ticker_save_path = \"/workspaces/backtesting/investment-portfolio-project/data/selected_tickers.json\"\n",
    "with open(ticker_save_path, 'w') as f:\n",
    "    json.dump(chosen_tickers, f)\n",
    "print(f\"ðŸ’¾ Selected tickers saved to {ticker_save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b344dbcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Process Each Ticker, Generate Individual Model Signals, and Create Ensemble Signal\n",
    "\n",
    "all_strategy_returns = {} # To store strategy returns for all tickers and models\n",
    "market_returns_df = pd.DataFrame() # To store market returns for all tickers\n",
    "\n",
    "# Loop through each chosen ticker\n",
    "for ticker in chosen_tickers:\n",
    "    print(f\"\\n--- Processing {ticker} ---\")\n",
    "    try:\n",
    "        # Fetch financial data\n",
    "        financial_data = fetch_financial_data(ticker, API_KEY)\n",
    "\n",
    "        # Determine news fetching date range based on financial data\n",
    "        news_start_date = financial_data.index.min()\n",
    "        news_end_date = financial_data.index.max()\n",
    "\n",
    "        # Fetch news sentiment\n",
    "        daily_sentiment = fetch_news_sentiment(ticker, API_KEY, news_start_date, news_end_date)\n",
    "        if daily_sentiment.empty:\n",
    "            print(f\"âš ï¸ Using mock sentiment for {ticker} due to API issues or no data.\")\n",
    "            # Fallback to mock data structure for consistency\n",
    "            mock_dates = pd.date_range(start=financial_data.index.min(), end=financial_data.index.max(), freq='D')\n",
    "            daily_sentiment = pd.DataFrame(0.0, index=mock_dates, columns=['daily_sentiment'])\n",
    "\n",
    "\n",
    "        # Add features and target to financial data\n",
    "        processed_data = add_features_and_target(financial_data, daily_sentiment)\n",
    "\n",
    "        # Train models and get predictions\n",
    "        predictions_df, test_index = train_and_predict_models(processed_data)\n",
    "\n",
    "        # Align market returns with the test_index\n",
    "        market_returns_ticker = processed_data['daily_return'].loc[test_index]\n",
    "        market_returns_df[ticker] = market_returns_ticker # Store for portfolio market returns\n",
    "\n",
    "        # Calculate strategy returns for each model\n",
    "        for col in predictions_df.columns:\n",
    "            signal_col = col # y_pred_rf, y_pred_best_rf, etc.\n",
    "            # Strategy return = shifted signal * market return\n",
    "            strategy_returns_ticker = predictions_df[signal_col].shift(1) * market_returns_ticker\n",
    "            strategy_returns_ticker.dropna(inplace=True) # Drop initial NaN from shift\n",
    "\n",
    "            all_strategy_returns[f\"{ticker}_{signal_col.replace('y_pred_', '')}\"] = strategy_returns_ticker\n",
    "\n",
    "        # --- Create an Ensemble Signal for the ticker ---\n",
    "        # Simple majority vote from ML models (RF, Best RF, XGB, SVM)\n",
    "        ensemble_ml_preds = predictions_df[['y_pred_rf', 'y_pred_best_rf', 'y_pred_xgb', 'y_pred_svm']]\n",
    "        # Sum the predictions, then apply sign. If sum > 0, majority positive (1); if sum < 0, majority negative (-1); else 0.\n",
    "        ensemble_signal_ticker = ensemble_ml_preds.sum(axis=1).apply(np.sign)\n",
    "\n",
    "        # Calculate ensemble strategy returns\n",
    "        ensemble_strategy_returns_ticker = ensemble_signal_ticker.shift(1) * market_returns_ticker\n",
    "        ensemble_strategy_returns_ticker.dropna(inplace=True)\n",
    "        all_strategy_returns[f\"{ticker}_ensemble\"] = ensemble_strategy_returns_ticker\n",
    "\n",
    "        print(f\"ðŸ“ˆ {ticker} processed. Generated signals for {len(predictions_df.columns)} models and an ensemble.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error processing {ticker}: {e}\")\n",
    "\n",
    "# Ensure all strategy return series have the same date range for optimization\n",
    "# Find the common index range across all generated strategy returns\n",
    "common_index = None\n",
    "for key, series in all_strategy_returns.items():\n",
    "    if common_index is None:\n",
    "        common_index = series.index\n",
    "    else:\n",
    "        common_index = common_index.intersection(series.index)\n",
    "\n",
    "# Filter all series to the common index\n",
    "for key, series in all_strategy_returns.items():\n",
    "    all_strategy_returns[key] = series.loc[common_index]\n",
    "\n",
    "market_returns_df = market_returns_df.loc[common_index] # Also align market returns\n",
    "\n",
    "print(\"\\nâœ… All tickers processed and individual/ensemble strategy returns generated.\")\n",
    "# Example of stored returns:\n",
    "# print(all_strategy_returns.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f2e597",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Ask user for weight between Sharpe and Return for Optimization\n",
    "\n",
    "try:\n",
    "    alpha = float(input(\"Enter the weight for Sharpe Ratio (between 0 and 1), e.g., 0.5: \"))\n",
    "    if not 0 <= alpha <= 1:\n",
    "        raise ValueError\n",
    "except:\n",
    "    print(\"Invalid input. Using default alpha = 0.5\")\n",
    "    alpha = 0.5\n",
    "\n",
    "print(f\"âš–ï¸ Optimization will use Î± = {alpha:.2f} â†’ {alpha:.0%} Sharpe, {(1-alpha):.0%} Return\")\n",
    "\n",
    "# A sample risk-free rate for Sharpe Ratio calculation\n",
    "RISK_FREE_RATE = 0.02\n",
    "print(f\"Using an annualized risk-free rate of {RISK_FREE_RATE:.2%} for Sharpe Ratio calculation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909a8ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Optimize Portfolio Weights for Each Strategy Type\n",
    "\n",
    "# Group strategies by their base model type\n",
    "strategy_sets = {\n",
    "    'RandomForest_Base': {k: v for k, v in all_strategy_returns.items() if '_rf' in k and '_best_rf' not in k},\n",
    "    'RandomForest_Best': {k: v for k, v in all_strategy_returns.items() if '_best_rf' in k},\n",
    "    'XGBoost': {k: v for k, v in all_strategy_returns.items() if '_xgb' in k},\n",
    "    'SVM': {k: v for k, v in all_strategy_returns.items() if '_svm' in k},\n",
    "    'Ensemble_ML': {k: v for k, v in all_strategy_returns.items() if '_ensemble' in k},\n",
    "    'SMA_Crossover': {k: v for k, v in all_strategy_returns.items() if '_crossover' in k},\n",
    "    'RSI_Classical': {k: v for k, v in all_strategy_returns.items() if '_rsi' in k},\n",
    "    'Market_Buy_Hold': {ticker: market_returns_df[ticker] for ticker in chosen_tickers} # Market returns for each ticker\n",
    "}\n",
    "\n",
    "optimized_weights = {}\n",
    "cumulative_portfolio_returns = {}\n",
    "portfolio_metrics = {}\n",
    "\n",
    "for name, strat_dict in strategy_sets.items():\n",
    "    if not strat_dict:\n",
    "        print(f\"Skipping {name}: No relevant strategy returns found.\")\n",
    "        continue\n",
    "\n",
    "    returns_df = pd.concat(strat_dict.values(), axis=1).fillna(0)\n",
    "    \n",
    "    # Ensure all columns in returns_df are numeric.\n",
    "    # This can be an issue if a strategy_returns_ticker was empty or malformed.\n",
    "    returns_df = returns_df.apply(pd.to_numeric, errors='coerce').fillna(0)\n",
    "\n",
    "\n",
    "    # If after fillna, all columns are 0, or there's only one column and it's all 0\n",
    "    if returns_df.empty or (returns_df.shape[1] == 1 and returns_df.iloc[:, 0].sum() == 0):\n",
    "        print(f\"Skipping optimization for {name}: No valid returns data.\")\n",
    "        continue\n",
    "\n",
    "    weights, obj_score = optimize_combined_strategy(returns_df, alpha, RISK_FREE_RATE)\n",
    "    portfolio_returns = returns_df.dot(weights)\n",
    "    \n",
    "    cumulative_portfolio_returns[name] = (1 + portfolio_returns).cumprod()\n",
    "    optimized_weights[name] = {col: weight for col, weight in zip(returns_df.columns, weights)} # Store weights with ticker names\n",
    "\n",
    "    # Calculate and store metrics for the optimized portfolio\n",
    "    ann_ret = annual_return(portfolio_returns)\n",
    "    sharpe = sharpe_ratio(portfolio_returns, risk_free_rate=RISK_FREE_RATE)\n",
    "    \n",
    "    portfolio_metrics[name] = {\n",
    "        'Annualized Return': ann_ret,\n",
    "        'Sharpe Ratio': sharpe,\n",
    "        'Final Cumulative Return': cumulative_portfolio_returns[name].iloc[-1]\n",
    "    }\n",
    "\n",
    "    print(f\"âœ… Optimized {name} Portfolio â€” Final Value: {cumulative_portfolio_returns[name].iloc[-1]:.4f}, Sharpe: {sharpe:.2f}\")\n",
    "\n",
    "print(\"\\n--- Summary of Optimized Portfolio Metrics ---\")\n",
    "for name, metrics in portfolio_metrics.items():\n",
    "    print(f\"\\n{name}:\")\n",
    "    for metric_name, value in metrics.items():\n",
    "        if \"Return\" in metric_name:\n",
    "            print(f\"  {metric_name}: {value:.2%}\")\n",
    "        else:\n",
    "            print(f\"  {metric_name}: {value:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d9fa012",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Visualize Performance\n",
    "\n",
    "plt.figure(figsize=(16, 9))\n",
    "for name, cum_returns in cumulative_portfolio_returns.items():\n",
    "    plt.plot(cum_returns, label=f'Optimized {name} Portfolio')\n",
    "\n",
    "plt.title('Cumulative Returns of Optimized Portfolios')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Cumulative Returns')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea10c3ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Display Optimized Weights\n",
    "\n",
    "print(\"\\n--- Optimized Portfolio Weights ---\")\n",
    "for portfolio_name, weights_dict in optimized_weights.items():\n",
    "    print(f\"\\nðŸ’¼ {portfolio_name} Portfolio Weights:\")\n",
    "    for ticker_strategy, weight in weights_dict.items():\n",
    "        if weight > 0.001: # Only show non-negligible weights\n",
    "            print(f\"  {ticker_strategy}: {weight:.2%}\")\n",
    "    if not any(w > 0.001 for w in weights_dict.values()):\n",
    "        print(\"  All weights effectively zero (or very small), no dominant asset/strategy selected.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f07e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Save Optimized Weights (Optional)\n",
    "import joblib\n",
    "\n",
    "weights_path = \"/workspaces/backtesting/investment-portfolio-project/models/optimized_ensemble_weights.pkl\"\n",
    "joblib.dump(optimized_weights, weights_path)\n",
    "print(f\"ðŸ’¾ Optimized weights saved to: {weights_path}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
