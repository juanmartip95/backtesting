{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b21ba612",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Libraries imported and global variables defined.\n",
      "✅ Financial Data fetched and prepared.\n",
      "             open   high    low  close     volume\n",
      "date                                             \n",
      "1999-11-01  80.00  80.69  77.37  77.62  2487300.0\n",
      "1999-11-02  78.00  81.69  77.31  80.25  3564600.0\n",
      "1999-11-03  81.62  83.25  81.00  81.50  2932700.0\n",
      "1999-11-04  82.06  85.37  80.62  83.62  3384700.0\n",
      "1999-11-05  84.62  88.37  84.00  88.31  3721500.0\n"
     ]
    },
    {
     "ename": "InvalidSchema",
     "evalue": "No connection adapters were found for '[https://www.alphavantage.co/query?function=NEWS_SENTIMENT&tickers=](https://www.alphavantage.co/query?function=NEWS_SENTIMENT&tickers=)AAPL&time_from=20240101T0000&time_to=20250531T2359&apikey=BKVEJWBZ2HWPABP0'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mInvalidSchema\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 86\u001b[39m\n\u001b[32m     83\u001b[39m time_to = \u001b[33m'\u001b[39m\u001b[33m20250531T2359\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;66;03m# Adjust to relevant date range for your data\u001b[39;00m\n\u001b[32m     85\u001b[39m news_url = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[https://www.alphavantage.co/query?function=NEWS_SENTIMENT&tickers=](https://www.alphavantage.co/query?function=NEWS_SENTIMENT&tickers=)\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mSYMBOL\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m&time_from=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtime_from\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m&time_to=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtime_to\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m&apikey=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mAPI_KEY\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m86\u001b[39m response = \u001b[43mrequests\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnews_url\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     87\u001b[39m json_data = response.json()\n\u001b[32m     89\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mfeed\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m json_data:\n\u001b[32m     90\u001b[39m     \u001b[38;5;66;03m# Extract relevant sentiment information\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/backtesting/my_env/lib/python3.12/site-packages/requests/api.py:73\u001b[39m, in \u001b[36mget\u001b[39m\u001b[34m(url, params, **kwargs)\u001b[39m\n\u001b[32m     62\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget\u001b[39m(url, params=\u001b[38;5;28;01mNone\u001b[39;00m, **kwargs):\n\u001b[32m     63\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[32m     64\u001b[39m \n\u001b[32m     65\u001b[39m \u001b[33;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     70\u001b[39m \u001b[33;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[32m     71\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mget\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/backtesting/my_env/lib/python3.12/site-packages/requests/api.py:59\u001b[39m, in \u001b[36mrequest\u001b[39m\u001b[34m(method, url, **kwargs)\u001b[39m\n\u001b[32m     55\u001b[39m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[32m     56\u001b[39m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[32m     57\u001b[39m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m sessions.Session() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/backtesting/my_env/lib/python3.12/site-packages/requests/sessions.py:589\u001b[39m, in \u001b[36mSession.request\u001b[39m\u001b[34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[39m\n\u001b[32m    584\u001b[39m send_kwargs = {\n\u001b[32m    585\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtimeout\u001b[39m\u001b[33m\"\u001b[39m: timeout,\n\u001b[32m    586\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mallow_redirects\u001b[39m\u001b[33m\"\u001b[39m: allow_redirects,\n\u001b[32m    587\u001b[39m }\n\u001b[32m    588\u001b[39m send_kwargs.update(settings)\n\u001b[32m--> \u001b[39m\u001b[32m589\u001b[39m resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    591\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/backtesting/my_env/lib/python3.12/site-packages/requests/sessions.py:697\u001b[39m, in \u001b[36mSession.send\u001b[39m\u001b[34m(self, request, **kwargs)\u001b[39m\n\u001b[32m    694\u001b[39m hooks = request.hooks\n\u001b[32m    696\u001b[39m \u001b[38;5;66;03m# Get the appropriate adapter to use\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m697\u001b[39m adapter = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_adapter\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    699\u001b[39m \u001b[38;5;66;03m# Start time (approximately) of the request\u001b[39;00m\n\u001b[32m    700\u001b[39m start = preferred_clock()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/backtesting/my_env/lib/python3.12/site-packages/requests/sessions.py:792\u001b[39m, in \u001b[36mSession.get_adapter\u001b[39m\u001b[34m(self, url)\u001b[39m\n\u001b[32m    789\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m adapter\n\u001b[32m    791\u001b[39m \u001b[38;5;66;03m# Nothing matches :-/\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m792\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m InvalidSchema(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNo connection adapters were found for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mInvalidSchema\u001b[39m: No connection adapters were found for '[https://www.alphavantage.co/query?function=NEWS_SENTIMENT&tickers=](https://www.alphavantage.co/query?function=NEWS_SENTIMENT&tickers=)AAPL&time_from=20240101T0000&time_to=20250531T2359&apikey=BKVEJWBZ2HWPABP0'"
     ]
    }
   ],
   "source": [
    "# Cell 1: Import Libraries (No Change from previous version)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from ta.momentum import RSIIndicator\n",
    "from ta.trend import MACD\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import classification_report, roc_auc_score, make_scorer\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder # Added LabelEncoder for XGBoost\n",
    "import seaborn as sns\n",
    "\n",
    "from alpha_vantage.timeseries import TimeSeries\n",
    "from alpha_vantage.techindicators import TechIndicators # Often used for news\n",
    "from alpha_vantage.foreignexchange import ForeignExchange # No, news is not here\n",
    "\n",
    "# For Alpha Vantage News & Sentiment API\n",
    "# While TimeSeries is also from alpha_vantage, the News and Sentiment endpoint is a bit distinct\n",
    "# You usually use the main TimeSeries object or a separate AlphaVantage object if available for news.\n",
    "# The simplest way is usually via ts.get_news_sentiment() if available or a direct API call if not.\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import requests # For direct API call if alpha_vantage library doesn't expose news directly\n",
    "\n",
    "# Load environment variables from .env\n",
    "load_dotenv()\n",
    "\n",
    "# Get the API key\n",
    "API_KEY = os.getenv(\"ALPHA_VANTAGE_API_KEY\")\n",
    "\n",
    "# Define target labels for multi-class classification\n",
    "LABELS = [-1, 0, 1]\n",
    "\n",
    "# Create a custom multi-class ROC AUC scorer for GridSearchCV\n",
    "multi_class_roc_auc_scorer = make_scorer(roc_auc_score, needs_proba=True, multi_class='ovr', labels=LABELS)\n",
    "\n",
    "print(\"✅ Libraries imported and global variables defined.\")\n",
    "\n",
    "\n",
    "# Cell 2: Fetch Financial Data and News Data from Alpha Vantage\n",
    "\n",
    "# --- Financial Data ---\n",
    "ts = TimeSeries(key=API_KEY, output_format='pandas')\n",
    "SYMBOL = 'AAPL' # Define the ticker symbol\n",
    "data, meta_data = ts.get_daily(symbol=SYMBOL, outputsize='full')\n",
    "\n",
    "# Clean and rename columns\n",
    "data.columns = ['open', 'high', 'low', 'close', 'volume']\n",
    "data.index = pd.to_datetime(data.index)\n",
    "data.sort_index(inplace=True)\n",
    "\n",
    "print(\"✅ Financial Data fetched and prepared.\")\n",
    "print(data.head())\n",
    "\n",
    "\n",
    "# --- News Data ---\n",
    "# Alpha Vantage News & Sentiment API:\n",
    "# This endpoint gives you news articles and their sentiment scores.\n",
    "# You might need to adjust the time range or make multiple calls for a long history.\n",
    "# For simplicity, we'll try to fetch recent news.\n",
    "# Example URL for Alpha Vantage News:\n",
    "# [https://www.alphavantage.co/query?function=NEWS_SENTIMENT&tickers=AAPL&time_from=20230101T0000&time_to=20231231T2359&apikey=YOUR_API_KEY](https://www.alphavantage.co/query?function=NEWS_SENTIMENT&tickers=AAPL&time_from=20230101T0000&time_to=20231231T2359&apikey=YOUR_API_KEY)\n",
    "\n",
    "news_data = []\n",
    "# Define a reasonable time range for fetching news, align with your financial data if possible\n",
    "# Note: Free Alpha Vantage API has limits on historical news data and rate limits.\n",
    "# You might need to iterate through smaller time windows or rely on mock data for extensive testing.\n",
    "# For demonstration, we'll fetch news for the last ~6 months from current date.\n",
    "# Adjust 'time_from' and 'time_to' as needed for your backtesting period.\n",
    "\n",
    "# To get news from the start of your financial data:\n",
    "# start_date_for_news = data.index.min().strftime('%Y%m%dT%H%M')\n",
    "# end_date_for_news = data.index.max().strftime('%Y%m%dT%H%M')\n",
    "\n",
    "# For this example, let's target a specific recent range or use a short period for demonstration\n",
    "# Due to API limits, fetching a \"full\" news history can be challenging on a free tier.\n",
    "# Let's set a realistic range for a quick run.\n",
    "# Current date: June 2025. Let's get news from Jan 2024 to May 2025 for a decent period.\n",
    "time_from = '20240101T0000'\n",
    "time_to = '20250531T2359' # Adjust to relevant date range for your data\n",
    "\n",
    "news_url = f\"[https://www.alphavantage.co/query?function=NEWS_SENTIMENT&tickers=](https://www.alphavantage.co/query?function=NEWS_SENTIMENT&tickers=){SYMBOL}&time_from={time_from}&time_to={time_to}&apikey={API_KEY}\"\n",
    "response = requests.get(news_url)\n",
    "json_data = response.json()\n",
    "\n",
    "if 'feed' in json_data:\n",
    "    # Extract relevant sentiment information\n",
    "    for article in json_data['feed']:\n",
    "        # Each article has 'overall_sentiment_score' and 'overall_sentiment_label'\n",
    "        # We also need the date. 'time_published' is usually in YYYYMMDDTHHMMSS format\n",
    "        pub_time = pd.to_datetime(article['time_published'], format='%Y%m%dT%H%M%S', errors='coerce')\n",
    "        if pd.isna(pub_time): # If format fails, try without seconds\n",
    "            pub_time = pd.to_datetime(article['time_published'], format='%Y%m%dT%H%M', errors='coerce')\n",
    "\n",
    "        # Ensure the article is about the target ticker\n",
    "        # Alpha Vantage tags relevant tickers in 'ticker_sentiment'\n",
    "        # This part requires careful parsing as 'ticker_sentiment' is a list of dicts.\n",
    "        target_ticker_sentiment = None\n",
    "        if 'ticker_sentiment' in article:\n",
    "            for ts_entry in article['ticker_sentiment']:\n",
    "                if ts_entry.get('ticker') == SYMBOL:\n",
    "                    target_ticker_sentiment = float(ts_entry.get('ticker_sentiment_score', 0.0))\n",
    "                    break\n",
    "        if target_ticker_sentiment is not None:\n",
    "             news_data.append({\n",
    "                 'date': pub_time.date(), # Get only the date part\n",
    "                 'sentiment': target_ticker_sentiment\n",
    "             })\n",
    "\n",
    "if news_data:\n",
    "    news_df = pd.DataFrame(news_data)\n",
    "    news_df['date'] = pd.to_datetime(news_df['date'])\n",
    "    news_df.set_index('date', inplace=True)\n",
    "    news_df.sort_index(inplace=True)\n",
    "\n",
    "    # Aggregate daily average sentiment\n",
    "    daily_avg_sentiment = news_df.groupby(news_df.index).mean().rename(columns={'sentiment': 'daily_sentiment'})\n",
    "\n",
    "    print(\"\\n✅ News Data fetched and daily sentiment aggregated.\")\n",
    "    print(daily_avg_sentiment.head())\n",
    "else:\n",
    "    print(\"\\n⚠️ Could not fetch news data or no relevant news found. Using mock data for sentiment.\")\n",
    "    # Fallback to mock data if API call fails or no data for the ticker.\n",
    "    # This is from your 05.5_newsarticles_signals.ipynb (simplified for a single ticker)\n",
    "    mock_data = {\n",
    "        'date': pd.to_datetime([\n",
    "            '2024-01-01', '2024-01-02', '2024-01-03', '2024-01-04', '2024-01-05',\n",
    "            '2024-01-08', '2024-01-09', '2024-01-10', '2024-01-11', '2024-01-12',\n",
    "            '2024-01-15', '2024-01-16', '2024-01-17', '2024-01-18', '2024-01-19',\n",
    "        ]),\n",
    "        'sentiment': [\n",
    "            0.8, 0.6, 0.1, 0.7, 0.9,\n",
    "            0.9, -0.3, -0.9, -0.7, 0.8,\n",
    "            -0.6, 0.7, 0.8, 0.3, -0.4,\n",
    "        ]\n",
    "    }\n",
    "    daily_avg_sentiment = pd.DataFrame(mock_data)\n",
    "    daily_avg_sentiment = daily_avg_sentiment.groupby('date')['sentiment'].mean().rename('daily_sentiment').to_frame()\n",
    "    daily_avg_sentiment.index = pd.to_datetime(daily_avg_sentiment.index)\n",
    "    daily_avg_sentiment.sort_index(inplace=True)\n",
    "    print(\"Using mock news sentiment data.\")\n",
    "    print(daily_avg_sentiment.head())\n",
    "\n",
    "\n",
    "# Cell 3: Feature Engineering (Updated to include sentiment features)\n",
    "# Create a copy for feature engineering\n",
    "feature_data = data.copy()\n",
    "\n",
    "# Calculate Daily Returns for feature\n",
    "feature_data['daily_return'] = feature_data['close'].pct_change()\n",
    "\n",
    "# Calculate RSI\n",
    "rsi_indicator = RSIIndicator(close=feature_data['close'], window=14)\n",
    "feature_data['rsi'] = rsi_indicator.rsi()\n",
    "\n",
    "# Calculate MACD\n",
    "macd_indicator = MACD(close=feature_data['close'])\n",
    "feature_data['macd'] = macd_indicator.macd()\n",
    "feature_data['macd_signal'] = macd_indicator.macd_signal()\n",
    "\n",
    "# --- Integrate News Sentiment Features ---\n",
    "# Join financial data with daily sentiment data\n",
    "# Use a left join to keep all financial dates and fill missing sentiment with 0 or NaN,\n",
    "# then handle NaNs as appropriate.\n",
    "feature_data = feature_data.join(daily_avg_sentiment, how='left')\n",
    "\n",
    "# Fill NaN sentiment values (e.g., no news on that day) with 0 or a reasonable neutral value\n",
    "# Or forward-fill/backward-fill if you assume sentiment persists.\n",
    "# For simplicity, let's fill with 0 (neutral) if no news.\n",
    "feature_data['daily_sentiment'].fillna(0, inplace=True)\n",
    "\n",
    "# Calculate rolling average and rolling standard deviation for sentiment\n",
    "# These help us understand the typical sentiment and its variability over time.\n",
    "# Use min_periods to ensure enough data points are available.\n",
    "feature_data['rolling_avg_sentiment_7d'] = feature_data['daily_sentiment'].rolling(window=7, min_periods=5).mean()\n",
    "feature_data['rolling_std_sentiment_30d'] = feature_data['daily_sentiment'].rolling(window=30, min_periods=10).std()\n",
    "\n",
    "# Define a threshold for detecting a \"shift\"\n",
    "# A shift is detected if the current daily sentiment deviates from the rolling average\n",
    "# by more than a certain number of standard deviations.\n",
    "STD_DEV_THRESHOLD = 1.5\n",
    "\n",
    "# Calculate the deviation from the rolling average\n",
    "feature_data['sentiment_deviation'] = abs(feature_data['daily_sentiment'] - feature_data['rolling_avg_sentiment_7d'])\n",
    "\n",
    "# Flag a sentiment shift if deviation is above the threshold * rolling_std_sentiment_30d\n",
    "# Ensure rolling std is not NaN when checking the condition\n",
    "feature_data['sentiment_shift_flag'] = np.where(\n",
    "    (feature_data['sentiment_deviation'] > (STD_DEV_THRESHOLD * feature_data['rolling_std_sentiment_30d'])) &\n",
    "    (feature_data['rolling_std_sentiment_30d'].notna()), # Ensure rolling std is not NaN\n",
    "    1, # Indicates a shift\n",
    "    0  # No shift\n",
    ")\n",
    "\n",
    "# Create a more nuanced sentiment signal\n",
    "# This can be a new feature for your ML models.\n",
    "# Example: 1 for significant positive sentiment, -1 for significant negative, 0 for neutral\n",
    "# You can refine these thresholds.\n",
    "feature_data['sentiment_signal'] = 0 # Default to neutral\n",
    "\n",
    "# Positive sentiment signal: if sentiment is high AND it's not a large negative shift\n",
    "feature_data.loc[(feature_data['daily_sentiment'] > 0.5) & (feature_data['sentiment_shift_flag'] == 0), 'sentiment_signal'] = 1\n",
    "# Negative sentiment signal: if sentiment is low AND it's not a large positive shift\n",
    "feature_data.loc[(feature_data['daily_sentiment'] < -0.5) & (feature_data['sentiment_shift_flag'] == 0), 'sentiment_signal'] = -1\n",
    "# Consider sentiment shifts (either positive or negative large shifts) as potentially influential\n",
    "feature_data.loc[(feature_data['sentiment_shift_flag'] == 1) & (feature_data['daily_sentiment'] > 0), 'sentiment_signal'] = 1\n",
    "feature_data.loc[(feature_data['sentiment_shift_flag'] == 1) & (feature_data['daily_sentiment'] < 0), 'sentiment_signal'] = -1\n",
    "\n",
    "# Drop rows with NaN values resulting from indicator calculations (financial and sentiment)\n",
    "feature_data.dropna(inplace=True)\n",
    "\n",
    "print(\"✅ Features engineered, including news sentiment features.\")\n",
    "print(feature_data.head())\n",
    "\n",
    "\n",
    "# Cell 4: Define Multi-Class Target Variable (-1 for Sell, 0 for Hold, 1 for Buy) (No Change)\n",
    "\n",
    "# Define return for the next period (shifted by -1 to predict the next day)\n",
    "feature_data['future_return'] = feature_data['close'].pct_change().shift(-1)\n",
    "\n",
    "# Define your thresholds for Buy/Sell\n",
    "BUY_THRESHOLD = 0.001\n",
    "SELL_THRESHOLD = -0.001\n",
    "\n",
    "# Initialize target column\n",
    "feature_data['target'] = 0\n",
    "\n",
    "# Assign Buy signals\n",
    "feature_data.loc[feature_data['future_return'] > BUY_THRESHOLD, 'target'] = 1\n",
    "\n",
    "# Assign Sell signals\n",
    "feature_data.loc[feature_data['future_return'] < SELL_THRESHOLD, 'target'] = -1\n",
    "\n",
    "# Drop the 'future_return' column after target definition to avoid look-ahead bias\n",
    "feature_data.drop(columns=['future_return'], inplace=True)\n",
    "\n",
    "# Drop any remaining rows with NaN values (e.g., last row after target shift)\n",
    "feature_data.dropna(inplace=True)\n",
    "\n",
    "print(\"✅ Multi-class target variable defined.\")\n",
    "print(feature_data['target'].value_counts())\n",
    "print(feature_data.head())\n",
    "\n",
    "\n",
    "# Cell 5: Prepare Data for Modeling (Updated features list)\n",
    "\n",
    "# Define features (X) and target (y)\n",
    "# Now include the new sentiment features!\n",
    "features = ['daily_return', 'rsi', 'macd', 'macd_signal',\n",
    "            'daily_sentiment', 'rolling_avg_sentiment_7d', 'sentiment_deviation', 'sentiment_shift_flag', 'sentiment_signal'] # Added sentiment features\n",
    "X = feature_data[features]\n",
    "y = feature_data['target']\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False, random_state=42)\n",
    "\n",
    "# Initialize StandardScaler for SVM.\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Convert scaled arrays back to DataFrames with original indices and columns\n",
    "X_train_scaled_df = pd.DataFrame(X_train_scaled, index=X_train.index, columns=X_train.columns)\n",
    "X_test_scaled_df = pd.DataFrame(X_test_scaled, index=X_test.index, columns=X_test.columns)\n",
    "\n",
    "print(\"✅ Data prepared for modeling, including sentiment features.\")\n",
    "print(f\"Train set shape: {X_train.shape}, Test set shape: {X_test.shape}\")\n",
    "print(\"Features used:\", X.columns.tolist())\n",
    "\n",
    "\n",
    "# Cell 6: Train and Evaluate RandomForestClassifier (No logical change, just rerun)\n",
    "\n",
    "print(\"--- RandomForestClassifier ---\")\n",
    "rf_model = RandomForestClassifier(random_state=42, class_weight='balanced')\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred_rf = rf_model.predict(X_test)\n",
    "y_proba_rf = rf_model.predict_proba(X_test)\n",
    "\n",
    "print(\"\\n📈 Classification Report (RandomForest):\")\n",
    "print(classification_report(y_test, y_pred_rf, labels=LABELS, zero_division=0))\n",
    "\n",
    "print(\"ROC AUC Score (RandomForest):\", roc_auc_score(y_test, y_proba_rf, multi_class='ovr', labels=LABELS))\n",
    "\n",
    "\n",
    "# Cell 7: Hyperparameter Tuning for RandomForestClassifier (GridSearchCV) (No logical change, just rerun)\n",
    "\n",
    "print(\"--- Hyperparameter Tuning (RandomForest with GridSearchCV) ---\")\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100],\n",
    "    'max_depth': [5, 10],\n",
    "    'min_samples_split': [5, 10],\n",
    "    'min_samples_leaf': [2, 4],\n",
    "    'class_weight': ['balanced']\n",
    "}\n",
    "\n",
    "rf_base = RandomForestClassifier(random_state=42)\n",
    "\n",
    "grid_search_rf = GridSearchCV(estimator=rf_base,\n",
    "                              param_grid=param_grid,\n",
    "                              cv=3,\n",
    "                              scoring=multi_class_roc_auc_scorer,\n",
    "                              verbose=2,\n",
    "                              n_jobs=-1)\n",
    "\n",
    "grid_search_rf.fit(X_train, y_train)\n",
    "\n",
    "print(\"🧠 Best Parameters (RandomForest):\")\n",
    "print(grid_search_rf.best_params_)\n",
    "\n",
    "best_rf_model = grid_search_rf.best_estimator_\n",
    "\n",
    "y_pred_best_rf = best_rf_model.predict(X_test)\n",
    "y_proba_best_rf = best_rf_model.predict_proba(X_test)\n",
    "\n",
    "print(\"\\n📈 Classification Report (Best RandomForest Model):\")\n",
    "print(classification_report(y_test, y_pred_best_rf, labels=LABELS, zero_division=0))\n",
    "\n",
    "print(f\"ROC AUC (Best RandomForest Model): {roc_auc_score(y_test, y_proba_best_rf, multi_class='ovr', labels=LABELS):.4f}\")\n",
    "\n",
    "\n",
    "# Cell 8: Train and Evaluate XGBClassifier (No logical change, just rerun with LabelEncoder)\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "print(\"\\n--- XGBClassifier ---\")\n",
    "\n",
    "le = LabelEncoder()\n",
    "y_train_encoded = le.fit_transform(y_train)\n",
    "y_test_encoded = le.transform(y_test)\n",
    "\n",
    "xgb_model = XGBClassifier(objective='multi:softmax',\n",
    "                          num_class=len(le.classes_),\n",
    "                          use_label_encoder=False,\n",
    "                          eval_metric='mlogloss',\n",
    "                          random_state=42)\n",
    "\n",
    "xgb_model.fit(X_train, y_train_encoded)\n",
    "\n",
    "y_pred_xgb_encoded = xgb_model.predict(X_test)\n",
    "y_pred_xgb = le.inverse_transform(y_pred_xgb_encoded)\n",
    "\n",
    "y_proba_xgb = xgb_model.predict_proba(X_test)\n",
    "\n",
    "print(\"\\n📈 Classification Report (XGBoost):\")\n",
    "print(classification_report(y_test, y_pred_xgb, labels=LABELS, zero_division=0))\n",
    "\n",
    "print(\"ROC AUC Score (XGBoost):\", roc_auc_score(y_test, y_proba_xgb, multi_class='ovr', labels=LABELS))\n",
    "\n",
    "\n",
    "# Cell 9: Train and Evaluate SVM (SVC) (No logical change, just rerun)\n",
    "\n",
    "print(\"\\n--- SVM (SVC) ---\")\n",
    "svm_model = SVC(probability=True, random_state=42, class_weight='balanced')\n",
    "svm_model.fit(X_train_scaled_df, y_train)\n",
    "\n",
    "y_pred_svm = svm_model.predict(X_test_scaled_df)\n",
    "y_proba_svm = svm_model.predict_proba(X_test_scaled_df)\n",
    "\n",
    "print(\"\\n📈 Classification Report (SVM):\")\n",
    "print(classification_report(y_test, y_pred_svm, labels=LABELS, zero_division=0))\n",
    "\n",
    "print(\"ROC AUC Score (SVM):\", roc_auc_score(y_test, y_proba_svm, multi_class='ovr', labels=LABELS))\n",
    "\n",
    "\n",
    "# Cell 10: Classical Signal Generation (SMA Crossover & RSI) (No Change, just rerun)\n",
    "\n",
    "feature_data['SMA_10'] = feature_data['close'].rolling(window=10).mean()\n",
    "feature_data['SMA_50'] = feature_data['close'].rolling(window=50).mean()\n",
    "\n",
    "feature_data['y_pred_crossover'] = np.where(feature_data['SMA_10'] > feature_data['SMA_50'], 1, 0)\n",
    "\n",
    "feature_data['y_pred_rsi'] = np.where(feature_data['rsi'] < 30, 1, 0)\n",
    "feature_data['y_pred_rsi'] = np.where(feature_data['rsi'] > 70, 0, feature_data['y_pred_rsi'])\n",
    "\n",
    "feature_data.dropna(inplace=True)\n",
    "\n",
    "print(\"✅ Classical signals generated.\")\n",
    "print(feature_data[['SMA_10', 'SMA_50', 'y_pred_crossover', 'rsi', 'y_pred_rsi']].head())\n",
    "\n",
    "\n",
    "# Cell 11: Prepare and Save Signal Data for Backtesting (Updated to save all predictions)\n",
    "\n",
    "# Assign predictions from models back to feature_data based on their respective test set indices\n",
    "feature_data['y_pred'] = 0\n",
    "feature_data.loc[X_test.index, 'y_pred'] = y_pred_rf\n",
    "\n",
    "feature_data['y_pred_best'] = 0\n",
    "feature_data.loc[X_test.index, 'y_pred_best'] = y_pred_best_rf\n",
    "\n",
    "feature_data['y_pred_xgb'] = 0\n",
    "feature_data.loc[X_test.index, 'y_pred_xgb'] = y_pred_xgb\n",
    "\n",
    "feature_data['y_pred_svm'] = 0\n",
    "feature_data.loc[X_test.index, 'y_pred_svm'] = y_pred_svm\n",
    "\n",
    "\n",
    "# Select relevant columns for signal_data.csv\n",
    "signal_data = feature_data[[\n",
    "    'close', 'y_pred', 'y_pred_best', 'y_pred_xgb', 'y_pred_svm',\n",
    "    'y_pred_crossover', 'y_pred_rsi'\n",
    "]].copy()\n",
    "\n",
    "signal_data.index.name = 'date'\n",
    "\n",
    "output_path = '/workspaces/backtesting/investment-portfolio-project/data/signal_data.csv'\n",
    "signal_data.to_csv(output_path)\n",
    "\n",
    "print(f\"✅ Señales añadidas correctamente. Archivo actualizado: {output_path}\")\n",
    "print(signal_data.tail())\n",
    "\n",
    "\n",
    "# Cell 12: Save Feature Data (Optional, but good practice) (No Change)\n",
    "\n",
    "feature_data_output_path = '/workspaces/backtesting/investment-portfolio-project/data/feature_data.csv'\n",
    "feature_data.to_csv(feature_data_output_path)\n",
    "print(f\"✅ Feature data saved to: {feature_data_output_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
